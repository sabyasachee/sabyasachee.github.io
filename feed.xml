<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sabyasachee.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sabyasachee.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-27T06:34:40+00:00</updated><id>https://sabyasachee.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Character Attribute Extraction</title><link href="https://sabyasachee.github.io/blog/2023/attribute-extraction/" rel="alternate" type="text/html" title="Character Attribute Extraction"/><published>2023-06-24T00:00:00+00:00</published><updated>2023-06-24T00:00:00+00:00</updated><id>https://sabyasachee.github.io/blog/2023/attribute-extraction</id><content type="html" xml:base="https://sabyasachee.github.io/blog/2023/attribute-extraction/"><![CDATA[<p><strong>Character attribute extraction is an information extraction task to find attribute frames describing characters.</strong></p> <p>Attribute frames describe the type, value, and context of the character attribute.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/attribute-frame.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/attribute-frame.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/attribute-frame.svg-1400.webp"/> <img src="/assets/img/posts/2023-06-24-attribute-extraction/attribute-frame.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Example of an attribute frame of the character <i>Alonzo Harris</i> from the movie <i>Training Day</i> </div> <p>The above picture shows an example of a character attribute frame.</p> <p>A character attribute frame consists of four elements – character, attribute-type, attribute-value, and passage.</p> <ol> <li> <p><strong>Character</strong> - The name of character about whom the attribute frame describes an attribute.</p> </li> <li> <p><strong>Attribute Type</strong> - The type of attribute described in the attribute frame, for example, age, gender, profession, appearance, personality, etc.</p> </li> <li> <p><strong>Attribute Value</strong> - The value of the attribute type portrayed by the character, for example, 17-years old, female, doctor, beautiful, introvert, etc.</p> </li> <li> <p><strong>Passage</strong> - The section, consisting of a single sentence to a few paragraphs, from the story document which describes the attribute of the character.</p> </li> </ol> <p>The biggest challenge to the character attribute extraction task is the absence of labeled datasets.</p> <p>We work around this by formulating character attribute extraction as a question answering task, evaluating the unsupervized responses of the model. <em>Please note that this only tests the model’s precision, and not its recall: how many gold attribute frames the model can extract.</em></p> <h2 id="classifications-of-attribute-types">Classifications of Attribute Types</h2> <p>Attribute types could be static or dynamic. Static attribute types such as gender and name seldom change, but dynamic attribute types such as attire, emotion, and possessions, can change frequently.</p> <p>Attribute types could also have small or large scope. Small-scope attribute types such as age and profession require a small context, usually a phrase or a single sentence, to be accurately determined. Large-scope attribute types require a larger context – several paragraphs or a full book chapter – for their resolution. The goal and personality of the character are examples of large-scope attribute-types.</p> <h2 id="question-answering-model-for-attribute-extraction">Question Answering Model for Attribute Extraction</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/attribute-extraction.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/attribute-extraction.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/attribute-extraction.svg-1400.webp"/> <img src="/assets/img/posts/2023-06-24-attribute-extraction/attribute-extraction.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The LLM-based question answering pipeline used to extract attribute frames </div> <p>The above picture shows the complete pipeline we use to extract, prompt, and evaluate the character attribute extraction task using a question answering approach.</p> <ol> <li> <p><strong>Find characters</strong> - Given the movie scripts (story documents), we apply coreference resolution to find the characters.</p> </li> <li> <p><strong>Find story passages</strong> - We apply screenplay parsing to find the sluglines, action descriptions, and speech segments of the movie script. We use the short action descriptions, containing between 25 to 200 words, to find the small-scope attribute types. We use multiple contiguous action and speech segments, containing between 200 to 496 words, to find the large-scope attribute types.</p> </li> <li> <p><strong>Find attribute types</strong> - In the absence of attribute taxonomies, we apply a data-driven approach to curate a list of attribute types. We use a <em>few-shot</em> prompt on the story passages to query which attribute types are described. We collate the responses and pick the attribute types forming the top-90% probability mass.</p> <p>We end up with 13 attribute types.</p> <table data-click-to-select="true" data-height="300" data-virtual-scroll-item-height="5px" data-pagination="false" data-toggle="table" data-url="/assets/json/attributes.json"> <thead> <tr> <th data-field="attribute" data-halign="center" data-align="center" data-sortable="true">Attribute Type</th> <th data-field="definition" data-halign="center" data-align="center" data-sortable="true">Definition</th> </tr> </thead> </table> </li> </ol> <p><br/></p> <ol> <li> <p><strong>Score implicitness</strong> - Having found the characters, passages, and attribute types, we can now query a large language model (LLM) to find the attribute values. Prompting a paid LLM service such as GPT will incur very high costs because the attribute distribution is very sparse. Therefore, we first estimate the likelihood of a passage to describe some attribute of the character. We call this the implicitness score.</p> <blockquote> <p>Implicitness is a score of how hard it is to find the attribute value of a specific attribute type and character from the passage.</p> </blockquote> <p>Consider the following passages.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Julia reappears from the kitchen holding a birthday cake, 17 candles on top. 
She brings it to John. He eyes her before blowing out the candles
</code></pre></div> </div> <p>To find the age of John, the model has to infer from the sentence that he is celebrating his birthday and is seventeen years old because his birthday cake contains seventeen candles. This is a hard or <em>highly implicit</em> example.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>John is celebrating his seventeenth birthday with his mother
</code></pre></div> </div> <p>Conversely, this is a much easier example for the model to find the age of John. Therefore, it is easy or <em>less implicit</em>.</p> <p>We find a crude estimate of implicitness by querying an open-source instruction-tuned LLM – <em>Flan</em>-T5 – on whether the passage describes the specific attribute type of the character. Additionally, we instruct the LLM to answer either in “yes” or “no”.</p> <p><strong>Implicitness is 1 - the word probability of “yes” in the LLM’s response.</strong></p> </li> <li> <p><strong>Prompt attribute value</strong> - We sample attribute type, character, and passage tuples based on the implicitness score and the movie genre. We end up with about 50 tuples per attribute type. Next, we apply different prompting approaches on GPT-3.5 to find the attribute value of these tuples.</p> <p>a. <strong>Zero-Shot</strong> - The zero-shot prompt consists of the task instruction, attribute type definition, and the example passage.</p> <p>b. <strong>Few-Shot</strong> - The few-shot prompt adds 6-8 exemplars to demonstrate the type of responses expected from the model.</p> <p>c. <strong>Chain-of-thought</strong> - The chain-of-thought prompt is similar to the few-shot prompt, except it modifies the responses in the exemplars to include an explanation before concluding with the answer. The explanation serves to summarize the relevant section of the passage that informs about the attribute value.</p> </li> </ol> <h2 id="results">Results</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/result-1.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/result-1.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/result-1.svg-1400.webp"/> <img src="/assets/img/posts/2023-06-24-attribute-extraction/result-1.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/result-2.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/result-2.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/result-2.svg-1400.webp"/> <img src="/assets/img/posts/2023-06-24-attribute-extraction/result-2.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/result-3.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/result-3.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-06-24-attribute-extraction/result-3.svg-1400.webp"/> <img src="/assets/img/posts/2023-06-24-attribute-extraction/result-3.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Performance of different prompting methods <i>(left)</i> and how it varies with the implicitness of the example <i>(middle)</i>. The <i>right</i> picture shows the results of error analysis. </div> <p>We evaluate the zero-shot, few-shot, and chain-of-thought prompting approaches on 680 examples (~50 examples per attribute type).</p> <p>Two trained human raters check the responses and judge if the predicted attribute value is correct. An expert adjudicates any disagreements. Future work should endeavor to curate a character attribute dataset for a more comprehensive evaluation.</p> <p><strong>Few-shot and Chain-of-Thought prompting methods perform better than Zero-shot in attribute extraction</strong> (left picture). However, there is no significant difference between the two.</p> <p>This does not mean that Chain-of-Thought is never useful. <strong>Chain-of-Thought approach performs better than Few-shot for hard examples</strong>. Our results (middle picture) show that Chain-of-Thought’s performance overtakes Few-Shot’s as the implicitness increases.</p> <p>The analysis of errors made by different prompting methods also reveals some interesting trends. The <em>Not-Found</em> error occurs when the model is unable to find the attribute value and provides a null response even though the example describes some relevant attribute value. The <em>Different-Attribute</em> error occurs when the model finds the attribute value of some attribute type different than the queried attribute type. <strong>Error analysis shows that Chain-of-Thought prompting makes more <em>Not-Found</em> and less <em>Different-Attribute</em> errors than Few-Shot</strong>.</p> <p>These results show that Chain-of-Thought might overextend its contextual inference and infer situations not implied by the example. On average, Chain-of-Thought does not improve performance in the attribute extraction task. However, as the examples become more implicit, Chain-of-Thought prompting is more effective than Few-Shot because it allows deeper understanding of the content. The results of error analysis also show that Chain-of-Thought is stricter than Few-Shot in providing a response and is more faithful to the queried attribute type.</p> <p>To conclude, an effective prompting approach is to apply Few-Shot prompting for the less implicit examples and change to Chain-of-Thought prompting when the examples become more implicit.</p>]]></content><author><name></name></author><category term="information-extraction"/><category term="question-answering"/><category term="movie-screenplays"/><summary type="html"><![CDATA[Can LLMs find character attributes?]]></summary></entry><entry><title type="html">Kalman Filter</title><link href="https://sabyasachee.github.io/blog/2020/kalman-filter/" rel="alternate" type="text/html" title="Kalman Filter"/><published>2020-06-15T00:00:00+00:00</published><updated>2020-06-15T00:00:00+00:00</updated><id>https://sabyasachee.github.io/blog/2020/kalman-filter</id><content type="html" xml:base="https://sabyasachee.github.io/blog/2020/kalman-filter/"><![CDATA[<div style="display:none"> $$ \usepackage{amsmath} \usepackage{amssymb} \newcommand{\x}{x_{k}} \newcommand{\xx}{x_{k+1}} \newcommand{\zz}{z_{k+1}} \newcommand{\Q}{Q_{k}} \newcommand{\R}{R_{k+1}} \newcommand{\F}{F_{k}} \newcommand{\G}{G_{k}} \newcommand{\H}{H_{k+1}} \newcommand{\postx}{\hat{x}_{k+1|k+1}} \newcommand{\prex}{\hat{x}_{k+1|k}} \newcommand{\postP}{P_{k+1|k+1}} \newcommand{\preP}{P_{k+1|k}} \newcommand{\Z}{Z_{k}} \newcommand{\ZZ}{Z_{k+1}} \newcommand{\K}{K_{k+1}} \newcommand{\KK}{K'_{k+1}} \newcommand{\var}[1]{E[(#1)(#1)^T]} \newcommand{\cvar}[2]{E[(#1)(#1)^T|#2]} \newcommand{\cvarbreak}[2]{\nonumber E[(#1)\\&amp;\nonumber\quad\quad(#1)^T|#2]} \newcommand{\cvarbreaktwo}[2]{\nonumber E[(#1)\\\nonumber &amp;\quad\quad(#1)^T|#2]} \newcommand{\argmin}[1]{\underset{#1}{\textit{argmin}}\quad} \newcommand{\part}{\frac{\partial}{\partial a_{ij}}} $$ </div> <h2 id="linear-system">Linear System</h2> <p>The figure below shows a discrete-time linear system.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2020-06-15-kalman-filter/linear-system-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2020-06-15-kalman-filter/linear-system-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2020-06-15-kalman-filter/linear-system-1400.webp"/> <img src="/assets/img/posts/2020-06-15-kalman-filter/linear-system.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Discrete-time linear system </div> <p>\(k\) represents the discrete time instants. \(\x\), \(u_k\) and \(z_k\) represent the state of the system, input to the system and measurement of the system at time \(k\) respectively. The system is governed by the following equations -</p> \[\begin{aligned} \xx &amp;= \F \x + \G u_k + w_k &amp; \text{(State Equation)} \\ \zz &amp;= \H \xx + v_{k+1} &amp; \text{(Sensor Equation)} \end{aligned}\] <p>\(w_k\) is the modeling noise of the state and \(v_k\) is the measurement noise of the sensors. \(x\), \(z\), \(w\) and \(v\) are random variables, and \(u\), \(F\), \(G\) and \(H\) are deterministic. The task of filtering is to obtain an estimate of state \(\x\), given observations \(z_1, z_2, ..., z_k\). The Kalman filter is a linear unbiased estimator which minimizes the mean squared error. We present the proof of Kalman filter here for a discrete-time linear system.</p> <h2 id="notations">Notations</h2> <p>We use the following notations:</p> \[\begin{aligned} Z_k &amp;= z_1, z_2, ..., z_k\\ \hat{x}_{k|j} &amp;= E[x_k | Z_j]\\ P_{k|j} &amp;= Var[x_k|Z_j] = \cvar{x_k - \hat{x}_{k|j}}{Z_j} \end{aligned}\] <h2 id="assumptions">Assumptions</h2> <p>We assume the following:</p> <ol> <li> <p>\(w_k\) and \(v_k\) are zero-mean white noise.</p> \[\begin{aligned} E[w_k] &amp;= \mathbf{0}\\ E[v_k] &amp;= \mathbf{0}\\ Cov[w_k,w_l] = E[w_k w_l^T] &amp;= \begin{cases} \Q &amp; k = l \\ \mathbf{0} &amp; k \neq l \end{cases}\\ Cov[v_k,v_l] = E[v_k v_l^T] &amp;= \begin{cases} R_k &amp; k = l \\ \mathbf{0} &amp; k \neq l \end{cases} \end{aligned}\] <p>\(\Q\) and \(R_k\) are symmetric positive-definite matrices.</p> </li> <li> <p>\(w_k\) and \(v_k\) are uncorrelated with \(\x\) and \(Z_k\).</p> </li> <li> <p>\(E[x_0] = x_{0|0}\) and \(Var[x_0] = \var{x_0 - x_{0|0}} = P_{0|0}\) are the given initial state conditions. \(x_0\) is uncorrelated with \(w_0\) and \(v_0\).</p> </li> </ol> <h2 id="kalman-filter">Kalman Filter</h2> <p>Kalman filter gives a recursive formula for the estimates in prediction and update steps.</p> <p><strong>Prediction</strong></p> \[\begin{aligned} \prex &amp;= \F \hat{x}_{k|k} + \G u_k \\ \preP &amp;= \F P_{k|k} \F^T + \Q \end{aligned}\] <p><strong>Update</strong></p> \[\begin{aligned} \K &amp;= \preP \H^T (\H \preP \H^T + \R)^{-1} \\ \postx &amp;= \prex + \K (\zz - \H \prex) \\ \postP &amp;= (I - \K \H) \preP \end{aligned}\] <p>The Kalman filter propagates the conditional mean and variance of the state, by expressing \(\postx\) and \(\postP\) in terms of \(\hat{x}_{k|k}\) and \(P_{k|k}\).</p> <p>\(\prex\) is the prior estimate of \(\xx\) before observing \(\zz\), and \(\postx\) is the posterior estimate of \(\xx\) after the measurement \(\zz\) of the state. Similarly \(\preP\) and \(\postP\) are the prior and posterior estimates of the variance of the state.</p> <p>\(\K\) is called the Kalman Gain at time \(k + 1\), because it weighs the information gained from the measurement \(\zz\) by taking its difference with the predicted measurement \(\H \prex\).</p> <p>We prove three lemmas in the next section, two of which involve taking the gradient of trace operations and the other concerns with uncorrelated random variables. We use those to prove the Kalman Filter equations in section.</p> <h2 id="lemmas">Lemmas</h2> <h3 id="lemma-1">Lemma 1</h3> <p>\(E[(X + Y)(X + Y)^T] = E[XX^T] + E[YY^T]\), if \(X\) and \(Y\) are uncorrelated, and one of \(E[X]\) or \(E[Y]\) is 0.</p> <details><summary>Proof</summary> <p><strong>Proof</strong></p> <p>\(X\) and \(Y\) are uncorrelated, therefore \(E[XY^T] = E[X]E[Y^T]\). <br/> One of \(E[X]\) or \(E[Y]\) equals 0, therefore \(E[XY^T] = 0\).</p> <p>Similarly, \(E[YX^T] = 0\).</p> <p>Expanding \(E[(X + Y)(X + Y)^T]\),</p> \[\begin{aligned} E[(X + Y)(X + Y)^T] &amp;= E[XX^T] + E[YY^T] + E[XY^T] + E[YX^T] \\ &amp;= E[XX^T] + E[YY^T] \qquad (E[XY^T] = E[YX^T] = 0) \end{aligned}\] <p>Therefore \(E[(X + Y)(X + Y)^T] = E[XX^T] + E[YY^T]\).</p> </details> <h3 id="lemma-2">Lemma 2</h3> \[\nabla_A \mathbf{Tr}(AB) = B^T\] <details><summary>Proof</summary> <p><strong>Proof</strong></p> <p>Let \(A = [a]_{n \times p}\) and \(B = [b]_{p \times n}\).</p> \[\begin{aligned} \mathbf{Tr}(AB) &amp;= \sum_{k=1}^{n} (AB)_{kk} = \sum_{k=1}^{n} \sum_{l=1}^{p} a_{kl} b_{lk} \end{aligned}\] <p>Taking gradient with respect to \(a_{ij}\),</p> \[\begin{aligned} \part \mathbf{Tr}(AB) &amp;= \part \sum_{k=1}^{n} \sum_{l=1}^{p} a_{kl} b_{lk} = \sum_{k=1}^{n} \sum_{l=1}^{p} \part a_{kl} b_{lk} \\ &amp;= \part a_{ij} b_{ji} = b_{ji} \end{aligned}\] <p>Therefore \(\nabla_A \mathbf{Tr}(AB) = B^T\).</p> </details> <h3 id="lemma-3">Lemma 3</h3> \[\nabla_A \mathbf{Tr}(ABA^T) = A(B + B^T) \overset{B = B^T}{=} 2AB\] <details><summary>Proof</summary> <p><strong>Proof</strong></p> <p>Let \(A = [a]_{n \times p}\) and \(B = [b]_{p \times p}\).</p> \[\begin{aligned} \mathbf{Tr}(ABA^T) &amp;= \sum_{k=1}^{n} (ABA^T)_{kk} \\ &amp;= \sum_{k=1}^{n} \sum_{l=1}^{p} a_{kl} (BA^T)_{lk}\\ &amp;= \sum_{k=1}^{n} \sum_{l=1}^{p} a_{kl} \sum_{r=1}^{p} b_{lr} (A^T)_{rk}\\ &amp;= \sum_{k=1}^{n} \sum_{l=1}^p \sum_{r=1}^p a_{kl} b_{lr} a_{kr} \end{aligned}\] <p>Taking gradient with respect to \(a_{ij}\),</p> \[\begin{aligned} \part \mathbf{Tr} (ABA^T) &amp;= \part \sum_{k=1}^{n} \sum_{l=1}^p \sum_{r=1}^p a_{kl} b_{lr} a_{kr} \\ &amp;= \sum_{k=1}^{n} \sum_{l=1}^p \sum_{r=1}^p \part a_{kl} b_{lr} a_{kr} \\ &amp;= \sum_{l=1}^p \sum_{r=1}^p \part a_{il} b_{lr} a_{ir} \\ &amp;= \part a_{ij}^2 b_{jj} + \sum_{l=1, l \neq j}^p \part a_{il} b_{lj} a_{ij} + \sum_{r=1, r \neq j}^p \part a_{ij} b_{jr} a_{ir} \\ &amp;= 2 a_{ij} b_{ij} + \sum_{l=1, l \neq j}^p a_{il}b_{lj} + \sum_{r=1,r \neq j}^p b_{jr} a_{ir} \\ &amp;= \sum_{l=1}^p a_{il} b_{lj} + \sum_{r=1}^p a_{ir}b_{jr} \end{aligned}\] <p>Therefore,</p> \[\begin{aligned} \nabla_A \mathbf{Tr} (ABA^T) &amp;= AB + AB^T \\ &amp;= 2AB \quad \text{if } B = B^T \end{aligned}\] </details> <h2 id="proof">Proof</h2> <p><strong>We prove the prediction and update equations of Kalman Filter.</strong></p> <p>Let us rewrite the equations governing the discrete-time linear system.</p> \[\begin{align} \xx &amp;= \F \x + \G u_k + w_k &amp; \text{(State Equation)} \label{state} \\ \zz &amp;= \H \xx + v_{k+1} &amp; \text{(Sensor Equation)} \label{sensor} \end{align}\] <p>We find \(\prex\) using the Kalman Filter state equation.</p> \[\begin{align} \nonumber \prex &amp;= E[x_{k+1} | Z_k] \\ \nonumber &amp;= E[\F \x + \G u_k + w_k | Z_k] &amp; \text{(using Eq \ref{state})} \\ \nonumber &amp;= \F E[\x | Z_k] + \G u_k + E[w_k | Z_k] &amp; \text{($u_k$ is not random)} \\ \nonumber &amp;= \F E[x_k | Z_k] + \G u_k &amp; (E[w_k] = 0) \\ &amp;= \F \hat{x}_{k|k} + \G u_k &amp; \text{(definition of $\hat{x}_{k|k}$)} \label{prex} \end{align}\] <p>We substitute \(\prex\) in the definition of \(\preP\) using equation \ref{prex}.</p> \[\begin{align} \nonumber \preP &amp;= \cvar{\xx - \prex}{Z_k} \\ \nonumber &amp;= \cvarbreaktwo{\F \x + \G u_k + w_k - \F \hat{x}_{k|k} - \G u_k}{Z_k} \\ \nonumber &amp; \qquad \text{(substituting $\xx$ and $\prex$ using Eq \ref{state} and Eq \ref{prex} resp.)} \\ \nonumber &amp;= \cvar{\F (\x - \hat{x}_{k|k}) + w_k}{Z_k} \end{align}\] <p>From definition, we know that \(\hat{x}_{k|k} = g(Z_k)\) for some function \(g\), and \(w_k\) is uncorrelated with \(Z_k\) and \(x_k\).</p> <p>Therefore \(w_k\) is uncorrelated with \(\F (\x - \hat{x}_{k|k})\). \(E[w_k] = 0\), and we can use Lemma 1.</p> \[\begin{align} \nonumber \preP &amp;= \cvar{\F (\x - \hat{x}_{k|k}) + w_k}{Z_k} \\ \nonumber &amp;= \F \cvar{\x - \hat{x}_{k|k}}{Z_k} \F^T + E[w_k w_k^T | Z_k] &amp; \text{(using Lemma 1)} \\ \nonumber &amp;= \F P_{k|k} \F^T + \Q &amp; \text{(definition of $P_{k|k}$ and $\Q$)} \end{align}\] <p>Kalman filter is an unbiased linear optimal estimator. This implies the following:</p> <ol> <li> <p><strong>Unbiased</strong> - The expected value of an unbiased estimator equals the expected value of the parameter. Therefore, \(E[\postx] = E[\xx]\) and \(E[\hat{x}_{k|k}] = E[\x]\).</p> </li> <li> <p><strong>Linear</strong> - A linear estimator means it is a linear combination of the observations, which in this case is \(Z_{k+1}\).</p> <p>\(\prex\) is a function of \(Z_k\). Therefore let \(\postx = \KK\prex + \K \zz\), for some matrix \(\KK\) and \(\K\).</p> </li> <li> <p><strong>Optimal</strong> - The estimator minimizes the mean squared error. Therefore, \(\postx = argmin \; E[(\xx - \postx)^T(\xx - \postx) | \ZZ]\).</p> </li> </ol> <p>We use these three conditions to find \(\K\) and \(\KK\).</p> \[\begin{align} \nonumber E[\xx] &amp;= E[\postx] &amp; \text{($\postx$ is unbiased)} \\ \nonumber &amp;= E[\KK\prex + \K \zz] &amp; \text{($\postx$ is a linear estimator)} \\ \nonumber \nonumber &amp;= \KK E[\F \hat{x}_{k|k} + \G u_k] + \K E[\H \xx + v_{k+1}] \\ \nonumber &amp;\qquad \text{(substituting $\zz$ and $\prex$ using Eq \ref{sensor} and Eq \ref{prex} resp.)} \\ \nonumber &amp;= \KK (\F E[\hat{x}_{k|k}] + \G u_k) + \K \H E[\xx] &amp; (E[v_{k+1}] = 0) \\ \nonumber &amp;= \KK (\F E[\x] + \G u_k) + \K \H E[\xx] &amp; \text{($\hat{x}_{k|k}$ is unbiased)} \\ \nonumber &amp;= \KK E[\F x_k + \G u_k + w_k] + \K \H E[\xx] &amp; (E[w_k] = 0) \\ \nonumber &amp;= \KK E[\xx] + \K \H E[\xx] &amp; \text{(using Eq \ref{state})} \\ \nonumber &amp;= (\KK + \K \H) E[\xx] \\ \nonumber I &amp;= \KK + \K \H \\ \KK &amp;= I - \K \H \label{kdash} \end{align}\] <p>Substituting \(\KK\) in the expression of \(\postx\),</p> \[\begin{align} \nonumber \postx &amp;= \KK \prex + \K \zz \\ \nonumber &amp;= (I - \K \H) \prex + \K \zz &amp; \text{(using Eq \ref{kdash})} \\ &amp;= \prex + \K (\zz - \H \prex) \label{postx} \end{align}\] <p>We substitute \(\postx\) in the definition of \(\postP\) using equation \ref{postx},</p> \[\begin{align} \nonumber \postP &amp;= \cvar{\xx - \postx}{Z_{k+1}} \\ &amp;= \cvarbreaktwo{\xx - \prex - \K (\zz - \H \prex)}{\ZZ} \\ \nonumber &amp;\qquad \text{(substituting $\xx$ using Eq \ref{postx})} \\ &amp;= \cvarbreaktwo{\xx - \prex - \K (\H \xx + v_{k+1} - \H \prex)}{\ZZ} \\ \nonumber &amp;\qquad \text{(substituting $\zz$ using Eq \ref{sensor})} \\ &amp;= \cvarbreak{(I - \K \H) (\xx - \prex) - \K v_{k+1}}{\ZZ} \end{align}\] <p>\(\postx = g(Z_{k+1})\) for some function \(g\), and \(v_{k+1}\) is uncorrelated with \(Z_{k+1}\) and \(x_{k+1}\). Therefore \(\K w_{k+1}\) is uncorrelated with \((I - \K \H) (\xx - \prex)\). \(E[v_{k+1}] = 0\). Therefore we can use Lemma 1.</p> \[\begin{align} \nonumber \postP &amp;= \cvarbreak{(I - \K \H) (\xx - \prex) - \K v_{k+1}}{\ZZ} \\ \nonumber &amp;= (I - \K \H) \cvar{\xx - \prex}{\ZZ} \\ \nonumber &amp;= (I - \K \H)^T + \K E[v_{k+1} v_{k+1}^T | \ZZ] \K^T &amp; \text{(using Lemma 1)} \\ \nonumber \nonumber &amp;= (I - \K \H) \preP (I - \K \H)^T + \K \R \K^T \\ &amp;\qquad \text{(definition of $\preP$ and $\R$)} \label{postP} \end{align}\] <p>We find \(\K\) by minimizing the conditional mean squared error of \(\postx\), \(L = E[(\xx - \postx)^T(\xx - \postx)|\ZZ]\).</p> <p>We express \(L\) as a trace of \(\postP\), and use equation \ref{postP} to write it in terms of \(\K\). We then minimize \(L\) and find \(\K\).</p> \[\begin{align} \nonumber L &amp;= E[(\xx - \postx)^T(\xx - \postx) | \ZZ] \\ \nonumber &amp;= \mathbf{Tr}(\cvar{\xx - \postx}{\ZZ}) \\ \nonumber &amp;= \mathbf{Tr}(\postP) &amp; \text{(definition of $\postP$)} \\ \nonumber &amp;= \mathbf{Tr}((I - \K \H) \preP (I - \K \H)^T + \K \R K^T) &amp; \text{(using Eq \ref{postP})} \\ \nonumber &amp;= \mathbf{Tr}(\; \preP - \preP \H^T \K^T - \K \H \preP \\ \nonumber &amp;\qquad + \K \H \preP \H^T \K^T + \K \R \K^T) \\ \nonumber &amp;= \mathbf{Tr}(\preP) - \mathbf{Tr}(\preP \H^T \K^T) - \mathbf{Tr}(\K \H \preP) \\ \nonumber &amp;\qquad + \mathbf{Tr}(\; \K (\H \preP \H^T + \R) \K^T) &amp; \text{($\mathbf{Tr}$ is a linear operator)} \\ \nonumber &amp;= \mathbf{Tr}(\preP) - \mathbf{Tr}(\K \H \preP) - \mathbf{Tr}(\K \H \preP) \\ \nonumber &amp;\qquad + \mathbf{Tr}(\; \K (\H \preP \H^T + \R) \K^T) &amp; (\mathbf{Tr}(A) = \mathbf{Tr}(A^T)) \\ \nonumber &amp;= \mathbf{Tr}(\preP) - 2 \; \mathbf{Tr}(\K \H \preP) \\ &amp;\qquad + \mathbf{Tr}(\; \K (\H \preP \H^T + \R) \K^T) \label{loss} \end{align}\] <p>Taking the gradient of \(L\) with respect to \(\K\) and setting it equal to 0,</p> \[\begin{align} \nonumber \nabla_{\K} L &amp;= 0 \\ \nonumber 0 &amp;= \nabla_{\K}\; (\mathbf{Tr}(\K (\H \preP \H^T + \R) \K^T) \\ \nonumber &amp;\qquad - 2\; \mathbf{Tr}(\K \H \preP) + \mathbf{Tr}(\preP)) &amp; \text{(using Eq \ref{loss})} \\ \nonumber &amp;= \nabla_{\K}\; (\mathbf{Tr}(\K (\H \preP \H^T + \R) \K^T) \\ \nonumber &amp;\qquad - 2\; \mathbf{Tr}(\K \H \preP)) \\ \nonumber &amp;= 2\;\K (\H \preP \H^T + \R) - 2\; \preP \H^T &amp; \text{(using Lemma 2 and 3)} \\ \therefore\quad \K &amp;= \preP \H^T (\H \preP \H^T + \R)^{-1} \label{kgain} \end{align}\] <p>We can simplify the expression of \(\postP\) in equation \ref{postP} using equation \ref{kgain}.</p> \[\begin{align} \nonumber \postP &amp;= (I - \K \H) \preP (I - \K \H)^T + \K \R \K^T \\ \nonumber &amp;= \preP - \preP \H^T \K^T - \K \H \preP \\ \nonumber &amp;\quad + \K \H \preP \H^T \K^T + \K \R \K^T \\ \nonumber &amp;= \preP - \preP \H^T \K^T - \K \H \preP \\ \nonumber &amp;\quad + \K (\H \preP \H^T + \R) \K^T \\ \nonumber &amp;= \preP - \preP \H^T \K^T - \K \H \preP \\ \nonumber &amp;\quad + \preP \H^T (\H \preP \H^T + \R)^{-1} (\H \preP \H^T + \R) \K^T \\ \nonumber &amp;\qquad \text{(substituing $\K$ using Eq \ref{kgain})} \\ \nonumber &amp;= \preP - \preP \H^T \K^T - \K \H \preP + \preP \H^T \K^T \\ \nonumber &amp;= \preP - \K \H \preP \\ \nonumber &amp;= (I - \K \H) \preP \end{align}\] <p>Arranging all results together,</p> \[\begin{aligned} \prex &amp;= \F \hat{x}_{k|k} + \G u_k \\ \preP &amp;= \F P_{k|k} \F^T + \Q \\ \K &amp;= \preP \H^T (\H \preP \H^T + \R)^{-1} \\ \postx &amp;= \prex + \K (\zz - \H \prex) \\ \postP &amp;= (I - \K \H) \preP \end{aligned}\]]]></content><author><name>Sabyasachee Baruah</name></author><category term="kalman-filter"/><summary type="html"><![CDATA[Proof of Kalman Filter]]></summary></entry></feed>